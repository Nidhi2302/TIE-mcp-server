{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIE MCP Server - Model Retraining and Data Experimentation\n",
    "\n",
    "This notebook provides an interactive environment for:\n",
    "- Creating custom datasets for TIE model training\n",
    "- Experimenting with different model configurations\n",
    "- Retraining models with new data\n",
    "- Evaluating model performance\n",
    "- Data preprocessing and augmentation\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Configuration](#setup)\n",
    "2. [Data Loading and Exploration](#data-loading)\n",
    "3. [Dataset Creation and Preprocessing](#dataset-creation)\n",
    "4. [Model Training Experiments](#model-training)\n",
    "5. [Model Evaluation and Comparison](#model-evaluation)\n",
    "6. [Advanced Data Framing](#advanced-data-framing)\n",
    "7. [Model Deployment](#model-deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "# TIE MCP imports\n",
    "from tie_mcp.config.settings import settings\n",
    "from tie_mcp.core.engine_manager import TIEEngineManager\n",
    "from tie_mcp.models.model_manager import ModelManager\n",
    "from tie_mcp.storage.database import DatabaseManager\n",
    "from tie_mcp.monitoring.metrics import MetricsCollector\n",
    "from tie_mcp.utils.logging import setup_logging, get_logger\n",
    "\n",
    "# Original TIE imports\n",
    "from tie_mcp.core.tie.engine import TechniqueInferenceEngine\n",
    "from tie_mcp.core.tie.matrix_builder import ReportTechniqueMatrixBuilder\n",
    "from tie_mcp.core.tie.constants import PredictionMethod\n",
    "from tie_mcp.core.tie.recommender import (\n",
    "    WalsRecommender,\n",
    "    BPRRecommender,\n",
    "    TopItemsRecommender,\n",
    ")\n",
    "from tie_mcp.core.tie.utils import get_mitre_technique_ids_to_names\n",
    "\n",
    "# Configure matplotlib for interactive plots\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Setup logging\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Current environment: {settings.environment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration {#data-loading}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load default dataset\n",
    "def load_tie_dataset(dataset_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load TIE dataset from JSON file\"\"\"\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load the default dataset\n",
    "dataset_path = project_root / \"data\" / \"datasets\" / \"combined_dataset_full_frequency.json\"\n",
    "dataset = load_tie_dataset(dataset_path)\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset['reports'])} reports\")\n",
    "print(f\"First report keys: {list(dataset['reports'][0].keys())}\")\n",
    "\n",
    "# Display dataset statistics\n",
    "reports = dataset['reports']\n",
    "all_techniques = set()\n",
    "report_technique_counts = []\n",
    "\n",
    "for report in reports:\n",
    "    techniques = list(report['mitre_techniques'].keys())\n",
    "    all_techniques.update(techniques)\n",
    "    report_technique_counts.append(len(techniques))\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Total reports: {len(reports)}\")\n",
    "print(f\"Unique techniques: {len(all_techniques)}\")\n",
    "print(f\"Average techniques per report: {np.mean(report_technique_counts):.2f}\")\n",
    "print(f\"Median techniques per report: {np.median(report_technique_counts):.2f}\")\n",
    "print(f\"Min techniques per report: {np.min(report_technique_counts)}\")\n",
    "print(f\"Max techniques per report: {np.max(report_technique_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribution of techniques per report\n",
    "axes[0, 0].hist(report_technique_counts, bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Number of Techniques per Report')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Techniques per Report')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Technique frequency analysis\n",
    "technique_counts = {}\n",
    "for report in reports:\n",
    "    for technique in report['mitre_techniques'].keys():\n",
    "        technique_counts[technique] = technique_counts.get(technique, 0) + 1\n",
    "\n",
    "# Top 20 most frequent techniques\n",
    "top_techniques = sorted(technique_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "techniques, counts = zip(*top_techniques)\n",
    "\n",
    "axes[0, 1].barh(range(len(techniques)), counts)\n",
    "axes[0, 1].set_yticks(range(len(techniques)))\n",
    "axes[0, 1].set_yticklabels(techniques, fontsize=8)\n",
    "axes[0, 1].set_xlabel('Frequency')\n",
    "axes[0, 1].set_title('Top 20 Most Frequent Techniques')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Technique frequency distribution (log scale)\n",
    "all_counts = list(technique_counts.values())\n",
    "axes[1, 0].hist(all_counts, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Technique Frequency')\n",
    "axes[1, 0].set_ylabel('Number of Techniques')\n",
    "axes[1, 0].set_title('Distribution of Technique Frequencies')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative distribution of technique coverage\n",
    "sorted_counts = sorted(all_counts, reverse=True)\n",
    "cumulative_pct = np.cumsum(sorted_counts) / np.sum(sorted_counts) * 100\n",
    "axes[1, 1].plot(range(1, len(cumulative_pct) + 1), cumulative_pct)\n",
    "axes[1, 1].set_xlabel('Number of Techniques (ranked by frequency)')\n",
    "axes[1, 1].set_ylabel('Cumulative Coverage (%)')\n",
    "axes[1, 1].set_title('Cumulative Technique Coverage')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some insights\n",
    "print(f\"\\nDataset Insights:\")\n",
    "print(f\"Top 10% of techniques cover {cumulative_pct[len(cumulative_pct)//10]:.1f}% of all occurrences\")\n",
    "print(f\"Techniques appearing only once: {sum(1 for c in all_counts if c == 1)} ({sum(1 for c in all_counts if c == 1)/len(all_counts)*100:.1f}%)\")\n",
    "print(f\"Most frequent technique: {top_techniques[0][0]} ({top_techniques[0][1]} occurrences)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Creation and Preprocessing {#dataset-creation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBuilder:\n",
    "    \"\"\"Helper class for building custom datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, enterprise_attack_path: str):\n",
    "        self.enterprise_attack_path = enterprise_attack_path\n",
    "        self.attack_techniques = get_mitre_technique_ids_to_names(enterprise_attack_path)\n",
    "        \n",
    "    def create_filtered_dataset(self, \n",
    "                               original_dataset: Dict[str, Any],\n",
    "                               min_frequency: int = 5,\n",
    "                               max_frequency: Optional[int] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Create a filtered dataset based on technique frequency\"\"\"\n",
    "        \n",
    "        reports = original_dataset['reports']\n",
    "        \n",
    "        # Count technique frequencies\n",
    "        tech_counts = {}\n",
    "        for report in reports:\n",
    "            for tech in report['mitre_techniques']:\n",
    "                tech_counts[tech] = tech_counts.get(tech, 0) + 1\n",
    "        \n",
    "        # Filter techniques by frequency\n",
    "        valid_techniques = set()\n",
    "        for tech, count in tech_counts.items():\n",
    "            if count >= min_frequency:\n",
    "                if max_frequency is None or count <= max_frequency:\n",
    "                    valid_techniques.add(tech)\n",
    "        \n",
    "        print(f\"Techniques before filtering: {len(tech_counts)}\")\n",
    "        print(f\"Techniques after filtering: {len(valid_techniques)}\")\n",
    "        \n",
    "        # Filter reports\n",
    "        filtered_reports = []\n",
    "        for report in reports:\n",
    "            filtered_techniques = {\n",
    "                tech: val for tech, val in report['mitre_techniques'].items()\n",
    "                if tech in valid_techniques\n",
    "            }\n",
    "            if filtered_techniques:  # Only keep reports with remaining techniques\n",
    "                new_report = report.copy()\n",
    "                new_report['mitre_techniques'] = filtered_techniques\n",
    "                filtered_reports.append(new_report)\n",
    "        \n",
    "        print(f\"Reports before filtering: {len(reports)}\")\n",
    "        print(f\"Reports after filtering: {len(filtered_reports)}\")\n",
    "        \n",
    "        return {\n",
    "            'reports': filtered_reports,\n",
    "            'metadata': {\n",
    "                'filter_type': 'frequency',\n",
    "                'min_frequency': min_frequency,\n",
    "                'max_frequency': max_frequency,\n",
    "                'original_report_count': len(reports),\n",
    "                'filtered_report_count': len(filtered_reports),\n",
    "                'original_technique_count': len(tech_counts),\n",
    "                'filtered_technique_count': len(valid_techniques)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def augment_dataset(self, dataset: Dict[str, Any], \n",
    "                       dropout_rate: float = 0.3) -> Dict[str, Any]:\n",
    "        \"\"\"Apply data augmentation using technique dropout\"\"\"\n",
    "        \n",
    "        original_reports = dataset['reports']\n",
    "        augmented_reports = original_reports.copy()\n",
    "        \n",
    "        for i, report in enumerate(original_reports):\n",
    "            techniques = list(report['mitre_techniques'].keys())\n",
    "            if len(techniques) > 2:  # Only augment if sufficient techniques\n",
    "                # Create version with some techniques dropped\n",
    "                num_to_keep = max(1, int(len(techniques) * (1 - dropout_rate)))\n",
    "                kept_techniques = np.random.choice(techniques, num_to_keep, replace=False)\n",
    "                \n",
    "                augmented_report = {\n",
    "                    'id': f\"aug_{i}\",\n",
    "                    'mitre_techniques': {tech: 1 for tech in kept_techniques},\n",
    "                    'metadata': {'augmented': True, 'original_id': report.get('id', i)}\n",
    "                }\n",
    "                augmented_reports.append(augmented_report)\n",
    "        \n",
    "        print(f\"Original reports: {len(original_reports)}\")\n",
    "        print(f\"Augmented reports: {len(augmented_reports)}\")\n",
    "        print(f\"Added: {len(augmented_reports) - len(original_reports)} reports\")\n",
    "        \n",
    "        return {\n",
    "            'reports': augmented_reports,\n",
    "            'metadata': {\n",
    "                **dataset.get('metadata', {}),\n",
    "                'augmented': True,\n",
    "                'dropout_rate': dropout_rate,\n",
    "                'augmented_count': len(augmented_reports) - len(original_reports)\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initialize dataset builder\n",
    "enterprise_attack_path = project_root / \"data\" / \"datasets\" / \"stix\" / \"enterprise-attack.json\"\n",
    "dataset_builder = DatasetBuilder(str(enterprise_attack_path))\n",
    "\n",
    "print(f\"Dataset builder initialized with {len(dataset_builder.attack_techniques)} ATT&CK techniques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filtered dataset focusing on more common techniques\n",
    "filtered_dataset = dataset_builder.create_filtered_dataset(\n",
    "    original_dataset=dataset,\n",
    "    min_frequency=5  # Techniques appearing in at least 5 reports\n",
    ")\n",
    "\n",
    "# Apply data augmentation\n",
    "augmented_dataset = dataset_builder.augment_dataset(\n",
    "    dataset=filtered_dataset,\n",
    "    dropout_rate=0.3\n",
    ")\n",
    "\n",
    "# Save the processed dataset\n",
    "processed_dataset_path = project_root / \"data\" / \"datasets\" / \"processed_dataset.json\"\n",
    "with open(processed_dataset_path, 'w') as f:\n",
    "    json.dump(augmented_dataset, f, indent=2)\n",
    "\n",
    "print(f\"\\nProcessed dataset saved to: {processed_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training Experiments {#model-training}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModelTrainer:\n",
    "    \"\"\"Simplified model trainer for experimentation\"\"\"\n",
    "    \n",
    "    def __init__(self, enterprise_attack_path: str):\n",
    "        self.enterprise_attack_path = enterprise_attack_path\n",
    "        self.experiment_results = []\n",
    "    \n",
    "    def train_and_evaluate_model(self, \n",
    "                                dataset_path: str,\n",
    "                                model_type: str = \"wals\",\n",
    "                                embedding_dimension: int = 4,\n",
    "                                test_ratio: float = 0.2,\n",
    "                                validation_ratio: float = 0.1) -> Dict[str, Any]:\n",
    "        \"\"\"Train and evaluate a single model configuration\"\"\"\n",
    "        \n",
    "        print(f\"Training {model_type} model with embedding dimension {embedding_dimension}\")\n",
    "        \n",
    "        try:\n",
    "            # Build data matrices\n",
    "            data_builder = ReportTechniqueMatrixBuilder(\n",
    "                combined_dataset_filepath=dataset_path,\n",
    "                enterprise_attack_filepath=self.enterprise_attack_path\n",
    "            )\n",
    "            \n",
    "            training_data, test_data, validation_data = data_builder.build_train_test_validation(\n",
    "                test_ratio, validation_ratio\n",
    "            )\n",
    "            \n",
    "            print(f\"Training data shape: {training_data.shape}\")\n",
    "            print(f\"Test data shape: {test_data.shape}\")\n",
    "            print(f\"Validation data shape: {validation_data.shape}\")\n",
    "            \n",
    "            # Create model\n",
    "            if model_type == \"wals\":\n",
    "                model = WalsRecommender(m=training_data.m, n=training_data.n, k=embedding_dimension)\n",
    "                prediction_method = PredictionMethod.DOT\n",
    "            elif model_type == \"bpr\":\n",
    "                model = BPRRecommender(m=training_data.m, n=training_data.n, k=embedding_dimension)\n",
    "                prediction_method = PredictionMethod.COSINE\n",
    "            elif model_type == \"top_items\":\n",
    "                model = TopItemsRecommender(m=training_data.m, n=training_data.n, k=embedding_dimension)\n",
    "                prediction_method = PredictionMethod.DOT\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "            \n",
    "            # Create TIE engine\n",
    "            tie = TechniqueInferenceEngine(\n",
    "                training_data=training_data,\n",
    "                validation_data=validation_data,\n",
    "                test_data=test_data,\n",
    "                model=model,\n",
    "                prediction_method=prediction_method,\n",
    "                enterprise_attack_filepath=self.enterprise_attack_path\n",
    "            )\n",
    "            \n",
    "            # Train with simple hyperparameters\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            if model_type == \"wals\":\n",
    "                mse = tie.fit(epochs=25, c=0.01, regularization_coefficient=0.001)\n",
    "            elif model_type == \"bpr\":\n",
    "                mse = tie.fit(epochs=20, learning_rate=0.001, regularization=0.01)\n",
    "            else:  # top_items\n",
    "                mse = tie.fit()\n",
    "            \n",
    "            training_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            # Evaluate model\n",
    "            metrics = {}\n",
    "            for k in [10, 20, 50]:\n",
    "                metrics[f'precision_at_{k}'] = tie.precision(k=k)\n",
    "                metrics[f'recall_at_{k}'] = tie.recall(k=k)\n",
    "                metrics[f'ndcg_at_{k}'] = tie.normalized_discounted_cumulative_gain(k=k)\n",
    "            \n",
    "            result = {\n",
    "                'model_type': model_type,\n",
    "                'embedding_dimension': embedding_dimension,\n",
    "                'training_time_seconds': training_time,\n",
    "                'mse': mse,\n",
    "                'metrics': metrics,\n",
    "                'dataset_info': {\n",
    "                    'training_reports': training_data.m,\n",
    "                    'test_reports': test_data.m,\n",
    "                    'validation_reports': validation_data.m,\n",
    "                    'num_techniques': training_data.n\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            self.experiment_results.append(result)\n",
    "            \n",
    "            print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "            print(f\"NDCG@20: {metrics['ndcg_at_20']:.4f}\")\n",
    "            print(f\"Precision@20: {metrics['precision_at_20']:.4f}\")\n",
    "            print(f\"Recall@20: {metrics['recall_at_20']:.4f}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training model: {str(e)}\")\n",
    "            return {'error': str(e), 'model_type': model_type}\n",
    "    \n",
    "    def compare_models(self, dataset_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Compare different model types\"\"\"\n",
    "        \n",
    "        model_types = [\"wals\", \"bpr\", \"top_items\"]\n",
    "        embedding_dims = [2, 4, 8]\n",
    "        \n",
    "        comparison_results = []\n",
    "        \n",
    "        for model_type in model_types:\n",
    "            for embedding_dim in embedding_dims:\n",
    "                print(f\"\\n--- Training {model_type} with embedding dim {embedding_dim} ---\")\n",
    "                result = self.train_and_evaluate_model(\n",
    "                    dataset_path=dataset_path,\n",
    "                    model_type=model_type,\n",
    "                    embedding_dimension=embedding_dim\n",
    "                )\n",
    "                \n",
    "                if 'error' not in result:\n",
    "                    comparison_results.append(result)\n",
    "        \n",
    "        if comparison_results:\n",
    "            # Convert to DataFrame for easier analysis\n",
    "            df_data = []\n",
    "            for result in comparison_results:\n",
    "                row = {\n",
    "                    'model_type': result['model_type'],\n",
    "                    'embedding_dimension': result['embedding_dimension'],\n",
    "                    'training_time': result['training_time_seconds'],\n",
    "                    'mse': result['mse']\n",
    "                }\n",
    "                # Add metrics\n",
    "                for metric_name, metric_value in result['metrics'].items():\n",
    "                    row[metric_name] = metric_value\n",
    "                \n",
    "                df_data.append(row)\n",
    "            \n",
    "            return pd.DataFrame(df_data)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SimpleModelTrainer(str(enterprise_attack_path))\n",
    "print(\"Model trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model comparison experiments\n",
    "print(\"Starting model comparison experiments...\")\n",
    "comparison_df = trainer.compare_models(str(processed_dataset_path))\n",
    "\n",
    "if not comparison_df.empty:\n",
    "    print(\"\\nExperiment Results Summary:\")\n",
    "    print(comparison_df.groupby('model_type')[['ndcg_at_20', 'precision_at_20', 'recall_at_20', 'training_time']].mean())\nelse:\n",
    "    print(\"No successful experiments to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Visualization {#model-evaluation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize experiment results if available\n",
    "if not comparison_df.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Model Comparison Results', fontsize=16)\n",
    "    \n",
    "    # NDCG@20 by model type and embedding dimension\n",
    "    pivot_ndcg = comparison_df.pivot(index='model_type', columns='embedding_dimension', values='ndcg_at_20')\n",
    "    sns.heatmap(pivot_ndcg, annot=True, fmt='.4f', ax=axes[0, 0], cmap='YlOrRd')\n",
    "    axes[0, 0].set_title('NDCG@20 by Model Type and Embedding Dimension')\n",
    "    \n",
    "    # Training time comparison\n",
    "    comparison_df.boxplot(column='training_time', by='model_type', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Training Time by Model Type')\n",
    "    axes[0, 1].set_ylabel('Training Time (seconds)')\n",
    "    \n",
    "    # Precision vs Recall scatter plot\n",
    "    for model_type in comparison_df['model_type'].unique():\n",
    "        model_data = comparison_df[comparison_df['model_type'] == model_type]\n",
    "        axes[1, 0].scatter(model_data['recall_at_20'], model_data['precision_at_20'], \n",
    "                          label=model_type, s=60, alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Recall@20')\n",
    "    axes[1, 0].set_ylabel('Precision@20')\n",
    "    axes[1, 0].set_title('Precision vs Recall@20')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Performance vs training time trade-off\n",
    "    axes[1, 1].scatter(comparison_df['training_time'], comparison_df['ndcg_at_20'], \n",
    "                      c=comparison_df['embedding_dimension'], s=60, alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Training Time (seconds)')\n",
    "    axes[1, 1].set_ylabel('NDCG@20')\n",
    "    axes[1, 1].set_title('Performance vs Training Time')\n",
    "    plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1], label='Embedding Dimension')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best configuration\n",
    "    best_config = comparison_df.loc[comparison_df['ndcg_at_20'].idxmax()]\n",
    "    print(f\"\\nBest Configuration:\")\n",
    "    print(f\"Model Type: {best_config['model_type']}\")\n",
    "    print(f\"Embedding Dimension: {best_config['embedding_dimension']}\")\n",
    "    print(f\"NDCG@20: {best_config['ndcg_at_20']:.4f}\")\n",
    "    print(f\"Precision@20: {best_config['precision_at_20']:.4f}\")\n",
    "    print(f\"Recall@20: {best_config['recall_at_20']:.4f}\")\n",
    "    print(f\"Training Time: {best_config['training_time']:.2f} seconds\")\nelse:\n",
    "    print(\"No experiment results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Data Analysis {#advanced-data-framing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze technique co-occurrence patterns\n",
    "def analyze_technique_cooccurrence(dataset: Dict[str, Any], top_n: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"Analyze which techniques commonly appear together\"\"\"\n",
    "    \n",
    "    reports = dataset['reports']\n",
    "    \n",
    "    # Get all unique techniques\n",
    "    all_techniques = set()\n",
    "    for report in reports:\n",
    "        all_techniques.update(report['mitre_techniques'].keys())\n",
    "    \n",
    "    all_techniques = sorted(list(all_techniques))\n",
    "    \n",
    "    # Create co-occurrence matrix\n",
    "    cooccurrence_matrix = np.zeros((len(all_techniques), len(all_techniques)))\n",
    "    technique_to_idx = {tech: i for i, tech in enumerate(all_techniques)}\n",
    "    \n",
    "    for report in reports:\n",
    "        report_techniques = list(report['mitre_techniques'].keys())\n",
    "        for i, tech1 in enumerate(report_techniques):\n",
    "            for tech2 in report_techniques[i:]:\n",
    "                idx1, idx2 = technique_to_idx[tech1], technique_to_idx[tech2]\n",
    "                cooccurrence_matrix[idx1, idx2] += 1\n",
    "                if idx1 != idx2:\n",
    "                    cooccurrence_matrix[idx2, idx1] += 1\n",
    "    \n",
    "    # Find most common pairs\n",
    "    cooccurrence_pairs = []\n",
    "    for i in range(len(all_techniques)):\n",
    "        for j in range(i + 1, len(all_techniques)):\n",
    "            if cooccurrence_matrix[i, j] > 1:  # Appears together more than once\n",
    "                cooccurrence_pairs.append({\n",
    "                    'technique_1': all_techniques[i],\n",
    "                    'technique_2': all_techniques[j],\n",
    "                    'cooccurrence_count': cooccurrence_matrix[i, j]\n",
    "                })\n",
    "    \n",
    "    cooccurrence_df = pd.DataFrame(cooccurrence_pairs)\n",
    "    cooccurrence_df = cooccurrence_df.sort_values('cooccurrence_count', ascending=False)\n",
    "    \n",
    "    return cooccurrence_df.head(top_n)\n",
    "\n",
    "# Analyze the processed dataset\n",
    "print(\"Analyzing technique co-occurrence patterns...\")\n",
    "cooccurrence_analysis = analyze_technique_cooccurrence(augmented_dataset, top_n=15)\n",
    "\n",
    "print(\"\\nTop 15 Technique Co-occurrence Patterns:\")\n",
    "for _, row in cooccurrence_analysis.iterrows():\n",
    "    print(f\"{row['technique_1']} + {row['technique_2']}: {row['cooccurrence_count']} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset quality metrics\n",
    "def analyze_dataset_quality(dataset: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze various quality metrics of the dataset\"\"\"\n",
    "    \n",
    "    reports = dataset['reports']\n",
    "    \n",
    "    # Basic statistics\n",
    "    report_sizes = [len(report['mitre_techniques']) for report in reports]\n",
    "    \n",
    "    # Technique frequency distribution\n",
    "    technique_counts = {}\n",
    "    for report in reports:\n",
    "        for technique in report['mitre_techniques']:\n",
    "            technique_counts[technique] = technique_counts.get(technique, 0) + 1\n",
    "    \n",
    "    # Sparsity analysis\n",
    "    total_possible_entries = len(reports) * len(technique_counts)\n",
    "    actual_entries = sum(len(report['mitre_techniques']) for report in reports)\n",
    "    sparsity = 1 - (actual_entries / total_possible_entries)\n",
    "    \n",
    "    # Coverage analysis\n",
    "    single_occurrence_techniques = sum(1 for count in technique_counts.values() if count == 1)\n",
    "    coverage_ratio = single_occurrence_techniques / len(technique_counts)\n",
    "    \n",
    "    quality_metrics = {\n",
    "        'total_reports': len(reports),\n",
    "        'total_unique_techniques': len(technique_counts),\n",
    "        'average_techniques_per_report': np.mean(report_sizes),\n",
    "        'median_techniques_per_report': np.median(report_sizes),\n",
    "        'std_techniques_per_report': np.std(report_sizes),\n",
    "        'sparsity': sparsity,\n",
    "        'single_occurrence_techniques': single_occurrence_techniques,\n",
    "        'coverage_ratio': coverage_ratio,\n",
    "        'most_frequent_technique': max(technique_counts.items(), key=lambda x: x[1]),\n",
    "        'least_frequent_techniques': min(technique_counts.values())\n",
    "    }\n",
    "    \n",
    "    return quality_metrics\n",
    "\n",
    "# Analyze quality of different datasets\n",
    "print(\"\\n=== Dataset Quality Analysis ===\")\n",
    "\n",
    "print(\"\\nOriginal Dataset:\")\n",
    "original_quality = analyze_dataset_quality(dataset)\n",
    "for key, value in original_quality.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nFiltered Dataset:\")\n",
    "filtered_quality = analyze_dataset_quality(filtered_dataset)\n",
    "for key, value in filtered_quality.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nAugmented Dataset:\")\n",
    "augmented_quality = analyze_dataset_quality(augmented_dataset)\n",
    "for key, value in augmented_quality.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Deployment Preparation {#model-deployment}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment results for deployment\n",
    "experiment_summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'dataset_info': {\n",
    "        'original_reports': len(dataset['reports']),\n",
    "        'filtered_reports': len(filtered_dataset['reports']),\n",
    "        'augmented_reports': len(augmented_dataset['reports']),\n",
    "        'processing_steps': [\n",
    "            'frequency_filtering_min_5',\n",
    "            'data_augmentation_dropout_30pct'\n",
    "        ]\n",
    "    },\n",
    "    'experiment_results': trainer.experiment_results if hasattr(trainer, 'experiment_results') else [],\n",
    "    'quality_metrics': {\n",
    "        'original': original_quality,\n",
    "        'filtered': filtered_quality,\n",
    "        'augmented': augmented_quality\n",
    "    },\n",
    "    'cooccurrence_analysis': cooccurrence_analysis.to_dict('records') if not cooccurrence_analysis.empty else []\n",
    "}\n",
    "\n",
    "# Save experiment summary\n",
    "summary_path = project_root / \"data\" / \"configs\" / \"experiment_summary.json\"\n",
    "summary_path.parent.mkdir(exist_ok=True)\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(experiment_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Experiment summary saved to: {summary_path}\")\n",
    "\n",
    "# Create deployment configuration\n",
    "if not comparison_df.empty:\n",
    "    best_config = comparison_df.loc[comparison_df['ndcg_at_20'].idxmax()]\n",
    "    \n",
    "    deployment_config = {\n",
    "        'recommended_model': {\n",
    "            'model_type': best_config['model_type'],\n",
    "            'embedding_dimension': int(best_config['embedding_dimension']),\n",
    "            'expected_performance': {\n",
    "                'ndcg_at_20': float(best_config['ndcg_at_20']),\n",
    "                'precision_at_20': float(best_config['precision_at_20']),\n",
    "                'recall_at_20': float(best_config['recall_at_20'])\n",
    "            }\n",
    "        },\n",
    "        'dataset_path': str(processed_dataset_path),\n",
    "        'enterprise_attack_path': str(enterprise_attack_path),\n",
    "        'training_parameters': {\n",
    "            'test_ratio': 0.2,\n",
    "            'validation_ratio': 0.1,\n",
    "            'auto_hyperparameter_tuning': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = project_root / \"data\" / \"configs\" / \"deployment_config.json\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(deployment_config, f, indent=2)\n",
    "    \n",
    "    print(f\"Deployment configuration saved to: {config_path}\")\n",
    "    print(\"\\nRecommended deployment configuration:\")\n",
    "    print(f\"Model Type: {deployment_config['recommended_model']['model_type']}\")\n",
    "    print(f\"Embedding Dimension: {deployment_config['recommended_model']['embedding_dimension']}\")\n",
    "    print(f\"Expected NDCG@20: {deployment_config['recommended_model']['expected_performance']['ndcg_at_20']:.4f}\")\nelse:\n",
    "    print(\"No successful experiments for deployment configuration\")\n",
    "\n",
    "print(\"\\n=== Notebook Complete ===\")\n",
    "print(\"This notebook has demonstrated:\")\n",
    "print(\"✓ Dataset loading and exploration\")\n",
    "print(\"✓ Data preprocessing and augmentation\")\n",
    "print(\"✓ Model training and evaluation\")\n",
    "print(\"✓ Performance comparison across configurations\")\n",
    "print(\"✓ Dataset quality analysis\")\n",
    "print(\"✓ Deployment preparation\")\n",
    "print(\"\\nYou can now use the TIE MCP Server with the optimized configurations!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}